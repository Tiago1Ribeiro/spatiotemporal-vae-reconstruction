{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 15:29:04.688611: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-12 15:29:04.799932: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 15:29:04.799970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 15:29:04.812136: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 15:29:04.844207: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 15:29:05.478576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import yaml\n",
    "from patoolib import extract_archive\n",
    "\n",
    "# Local Module Imports\n",
    "sys.path.append(\"../src\")  # adds source code directory\n",
    "from utils import wkt_to_masc\n",
    "from log_setup import logger\n",
    "from polygon_handle import masks_to_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "base_dir = os.path.dirname(current_dir)\n",
    "dataset_dir = os.path.join(base_dir, \"dataset\")\n",
    "data_dir = os.path.join(base_dir, \"data\")\n",
    "config_file = os.path.join(base_dir, \"config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset extration and config file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there isn´t unrar installed, install it with: sudo apt-get install unrar (linux)\n",
    "# for windows, install it from: https://www.rarlab.com/rar_add.htm (unrarw32.exe)\n",
    "\n",
    "dataset_path = os.path.join(dataset_dir, \"BurnedAreaUAV_dataset\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    extract_archive(\n",
    "        os.path.join(dataset_dir, \"BurnedAreaUAV_dataset_v1.rar\"),\n",
    "        program=\"unrar\",\n",
    "        outdir=dataset_dir,\n",
    "    )\n",
    "    os.remove(os.path.join(dataset_dir, \"BurnedAreaUAV_dataset_v1.rar\"))\n",
    "\n",
    "with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output directory struture creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"CVAE\"]\n",
    "keys = [ \"extrapol\"]\n",
    "sub_keys_1 = [\"10\", \"30\"]\n",
    "sub_keys_2 = [\"full\", \"sampled\",\"unet\"]\n",
    "sub_keys_3 = [\"PNG\", \"WKT\"]\n",
    "\n",
    "model_names = [\"CVAE\"]\n",
    "keys = [ \"extrapol\"]\n",
    "sub_keys_1 = [\"10\", \"30\"]\n",
    "sub_keys_2 = [\"unet\"]\n",
    "sub_keys_3 = [\"PNG\", \"WKT\"]\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    for key in keys if model_name == \"CVAE\" else [\"interpol\"]:\n",
    "        if key == \"interpol\":\n",
    "            for sub_key, sub_sub_key in itertools.product(sub_keys_2, sub_keys_3):\n",
    "                # Create the directories\n",
    "                dir_path = os.path.join(\n",
    "                    base_dir, \"outputs\", model_name, key, sub_key, sub_sub_key\n",
    "                )\n",
    "                os.makedirs(dir_path, exist_ok=True)\n",
    "                # Create a .gitkeep file in the directory\n",
    "                with open(os.path.join(dir_path, \".gitkeep\"), \"w\") as f:\n",
    "                    pass\n",
    "        else:\n",
    "            for sub_key_1, sub_key_2, sub_sub_key in itertools.product(sub_keys_1, sub_keys_2, sub_keys_3):\n",
    "                # Create the directories\n",
    "                dir_path = os.path.join(\n",
    "                    base_dir, \"outputs\", model_name, key, sub_key_1, sub_key_2, sub_sub_key\n",
    "                )\n",
    "                os.makedirs(dir_path, exist_ok=True)\n",
    "                # Create a .gitkeep file in the directory\n",
    "                with open(os.path.join(dir_path, \".gitkeep\"), \"w\") as f:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masks directory creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks_dir = os.path.join(base_dir, config[\"data\"][\"train_dir\"], \"masks\")\n",
    "if not os.path.exists(train_masks_dir):\n",
    "    os.makedirs(train_masks_dir)\n",
    "\n",
    "test_masks_dir = os.path.join(base_dir, config[\"data\"][\"test_dir\"], \"masks\")\n",
    "if not os.path.exists(test_masks_dir):\n",
    "    os.makedirs(test_masks_dir)\n",
    "\n",
    "train_sampled_masks_dir = os.path.join(\n",
    "    base_dir, config[\"data\"][\"train_sampled_dir\"], \"masks\"\n",
    ")\n",
    "if not os.path.exists(train_sampled_masks_dir):\n",
    "    os.makedirs(train_sampled_masks_dir)\n",
    "\n",
    "unet_masks_dir = os.path.join(base_dir, config[\"data\"][\"unet_gen_dir\"], \"masks\")\n",
    "if not os.path.exists(unet_masks_dir):\n",
    "    os.makedirs(unet_masks_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WKT files to masks connversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "    --------------------------------------\n",
      "    # \u001b[1mProperties of the resulting masks\u001b[0m\n",
      "    # Width: 512, Height: 512\n",
      "    # Number of masks to create: 226\n",
      "    --------------------------------------\n",
      "    \n",
      "100%|██████████| 226/226 [00:00<00:00, 628.07it/s]\n",
      "INFO - \n",
      "    --------------------------------------\n",
      "    # \u001b[1mProperties of the resulting masks\u001b[0m\n",
      "    # Width: 512, Height: 512\n",
      "    # Number of masks to create: 23\n",
      "    --------------------------------------\n",
      "    \n",
      "100%|██████████| 23/23 [00:00<00:00, 658.00it/s]\n",
      "INFO - \n",
      "    --------------------------------------\n",
      "    # \u001b[1mProperties of the resulting masks\u001b[0m\n",
      "    # Width: 512, Height: 512\n",
      "    # Number of masks to create: 13\n",
      "    --------------------------------------\n",
      "    \n",
      "100%|██████████| 13/13 [00:00<00:00, 683.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# full training set\n",
    "wkt_to_masc(\n",
    "    wkt_file=os.path.join(base_dir, config[\"data\"][\"train_wkt\"]),\n",
    "    images_path=train_masks_dir,\n",
    "    orig_dims=config[\"data\"][\"original_vid_dims\"][::-1],\n",
    "    height=config[\"data\"][\"input_size\"][0],\n",
    "    width=config[\"data\"][\"input_size\"][1],\n",
    ")\n",
    "\n",
    "# test set\n",
    "wkt_to_masc(\n",
    "    wkt_file=os.path.join(base_dir, config[\"data\"][\"test_wkt\"]),\n",
    "    images_path=test_masks_dir,\n",
    "    orig_dims=config[\"data\"][\"original_vid_dims\"][::-1],\n",
    "    height=config[\"data\"][\"input_size\"][0],\n",
    "    width=config[\"data\"][\"input_size\"][1],\n",
    ")\n",
    "\n",
    "# sampled training set\n",
    "wkt_to_masc(\n",
    "    wkt_file=os.path.join(base_dir, config[\"data\"][\"sampled_masks_wkt\"]),\n",
    "    images_path=train_sampled_masks_dir,\n",
    "    orig_dims=config[\"data\"][\"original_vid_dims\"][::-1],\n",
    "    height=config[\"data\"][\"input_size\"][0],\n",
    "    width=config[\"data\"][\"input_size\"][1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net data distance-based sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "BASE_DIR = os.path.dirname(current_dir)\n",
    "config_file = os.path.join(BASE_DIR, \"config.yml\")\n",
    "\n",
    "with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Converting masks to polygons...\n",
      "INFO - Saved polygons to /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/data/unet_generated/WKT/masks.wkt\n"
     ]
    }
   ],
   "source": [
    "# List of generated frames paths\n",
    "msks_paths = sorted(glob(os.path.join(BASE_DIR, \"data/unet_generated/masks\", \"*.png\")))\n",
    "\n",
    "# Convert the masks to polygons and save them as a WKT file\n",
    "masks_to_polygons(\n",
    "    msks_paths,\n",
    "    out_dim=tuple(config[\"data\"][\"original_vid_dims\"]),\n",
    "    save_path=os.path.join(BASE_DIR, \"data/unet_generated/WKT\", \"masks.wkt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jaccard\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "\n",
    "def calculate_distances(polygons: list, out_shape: tuple = (720, 1280)):\n",
    "    \"\"\"\n",
    "    Calculates the Jaccard distance between binary segmentation masks of the\n",
    "    first and subsequent polygons\n",
    "\n",
    "    Args:\n",
    "        polygons (list): A list of polygons represented as a list of coordinate\n",
    "        tuples.\n",
    "        out_shape (tuple): The shape of the output rasterized mask.\n",
    "        Default is (720, 1280).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with two keys - \"Jaccard distance\". The values\n",
    "        for each key are lists containing the distance values between the\n",
    "        first and subsequent polygons\n",
    "\n",
    "    \"\"\"\n",
    "    distances = {\"Jaccard distance\": []}\n",
    "    masks = rasterize(polygons, out_shape)\n",
    "    mask_t0 = masks[0]\n",
    "\n",
    "    # Calculate Jaccard distances\n",
    "    for i in tqdm(range(1, len(polygons))):\n",
    "        mask_tn = masks[i]\n",
    "        if np.all(mask_tn == 0) or np.all(mask_t0 == 0):\n",
    "            distances[\"Jaccard distance\"].append(0)\n",
    "        else:\n",
    "            jaccard_distance = jaccard(mask_t0.flatten(), mask_tn.flatten())\n",
    "            distances[\"Jaccard distance\"].append(jaccard_distance)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def gen_similar_poly_samples(polygons, threshold=0.15, out_shape=(720, 1280)):\n",
    "    \"\"\"\n",
    "    Generate a set of samples from a list of polygons based on their similarity.\n",
    "\n",
    "    Args:\n",
    "        polygons (list): A list of polygons represented as lists of (x, y) tuples.\n",
    "        threshold (float): The Jaccard distance threshold for creating a new sample.\n",
    "            Defaults to 0.15.\n",
    "        out_shape (tuple): The output shape of the rasterized polygons.\n",
    "            Defaults to (720, 1280).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with two keys: \"index\" and \"Jaccard distance\".\n",
    "            The \"index\" value is the index of the polygon in the input list\n",
    "            for each sample.\n",
    "\n",
    "    \"\"\"\n",
    "    # Instantiate dictionary to store index and distance values\n",
    "    samples = {\"index\": [], \"Jaccard distance\": []}\n",
    "    idx = 0\n",
    "    while idx < len(polygons) - 1:\n",
    "        # Rasterize the first polygon\n",
    "        first_mask = rasterize([polygons[idx]], out_shape)\n",
    "        jaccard_distance = 0.0\n",
    "        while jaccard_distance < threshold and idx < len(polygons) - 1:\n",
    "            idx += 1\n",
    "            # Rasterize the subsequent polygon\n",
    "            second_mask = rasterize([polygons[idx]], out_shape)\n",
    "            # Calculate Jaccard distance\n",
    "            jaccard_distance = jaccard(first_mask.flatten(), second_mask.flatten())\n",
    "\n",
    "        # Append index and distance to dictionary\n",
    "        samples[\"index\"].append(idx)\n",
    "        samples[\"Jaccard distance\"].append(jaccard_distance)\n",
    "        print(f\"Index: {idx}, Jaccard distance: {jaccard_distance:.4f}  \", end=\"\\r\")\n",
    "\n",
    "    logger.info(f\"Number of resulting samples: {len(samples['index'])}\")\n",
    "\n",
    "    return samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
