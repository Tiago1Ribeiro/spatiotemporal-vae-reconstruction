{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Tensorflow Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Lambda, Input, Dense\n",
    "from tensorflow.keras.layers import MaxPooling2D, Flatten, Reshape, Concatenate\n",
    "from tensorflow.keras.layers import SeparableConv2D, Conv2DTranspose\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "disable_eager_execution()\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Local Module Imports\n",
    "sys.path.append(\"../src\")  # adds source code directory\n",
    "from utils import load_preprocess_mask, label_to_frame, frame_to_label\n",
    "from utils import frames_to_video, save_history\n",
    "from visualization import plot_learning_curves\n",
    "from polygon_handle import masks_to_polygons\n",
    "from log_setup import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "DATA: \"full\" (full dataset), \"sampled\" (distance sampled dataset) \n",
    "        or \"unet\" (unet generated dataset)\n",
    "MODE: \"interpol\" (interpolation) or \"extrapol\" (extrapolation)\n",
    "MODEL: \"CVAE\"\n",
    "PERCENTAGE: percentage of training data to be used for training\n",
    "LAST_FRAME: last frame number of the video\n",
    "\"\"\"\n",
    "\n",
    "DATA = \"unet\"\n",
    "MODE = \"extrapol\"\n",
    "MODEL = \"CVAE\"\n",
    "PERCENTAGE = 70\n",
    "LAST_FRAME = 22500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Data: unet, Mode: extrapol, Model: CVAE Percentage: 70%,\n",
      "Output directory: /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/outputs/CVAE/extrapol/70/unet\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "BASE_DIR = os.path.dirname(current_dir)\n",
    "dataset_dir = os.path.join(BASE_DIR, \"dataset\")\n",
    "data_dir = os.path.join(BASE_DIR, \"data\")\n",
    "config_file = os.path.join(BASE_DIR, \"config.yml\")\n",
    "\n",
    "# Output PNG directory\n",
    "if MODE == \"extrapol\":\n",
    "    output_dir = os.path.join(BASE_DIR, \"outputs\", \"CVAE\", MODE, str(PERCENTAGE), DATA)\n",
    "    logger.info(\n",
    "        f\"Data: {DATA}, Mode: {MODE}, Model: {MODEL} Percentage: {PERCENTAGE}%,\\nOutput directory: {output_dir}\"\n",
    "    )\n",
    "elif MODE == \"interpol\":\n",
    "    output_dir = os.path.join(BASE_DIR, \"outputs\", \"CVAE\", MODE, DATA)\n",
    "    logger.info(\n",
    "        f\"\\nData: {DATA}, Mode: {MODE}, Model: {MODEL}\\nOutput directory: {output_dir}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "if DATA == \"full\":\n",
    "    train_dir = os.path.join(BASE_DIR, config[\"data\"][\"full\"][\"train_dir\"], \"masks\")\n",
    "    # sort the paths\n",
    "    train_paths = sorted(glob(os.path.join(train_dir, \"*.png\")))\n",
    "    # extract labels from the paths\n",
    "    train_labels = [\n",
    "        int(os.path.basename(m).split(\"_\")[1].split(\".\")[0]) * 100 for m in train_paths\n",
    "    ]\n",
    "    epochs = config[\"CVAE\"][\"epoch\"]\n",
    "\n",
    "elif DATA == \"sampled\":\n",
    "    sampled_masks_txt_path = os.path.join(BASE_DIR, config[\"data\"][\"sampled_masks_txt\"])\n",
    "    with open(sampled_masks_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        polygons = f.readlines()\n",
    "        # extract indexes\n",
    "    indexes = [int(polygon.split(\",\")[0]) for polygon in polygons]\n",
    "    train_dir = os.path.join(BASE_DIR, config[\"data\"][\"sampled\"][\"train_dir\"], \"masks\")\n",
    "    train_paths = sorted(glob(os.path.join(train_dir, \"*.png\")))\n",
    "    train_labels = [100 * i for i in indexes]\n",
    "    epochs = config[\"CVAE\"][\"epoch\"]\n",
    "\n",
    "elif DATA == \"unet\":\n",
    "    train_dir = os.path.join(BASE_DIR, config[\"data\"][\"unet\"][\"train_dir\"], \"masks\")\n",
    "    train_paths = sorted(glob(os.path.join(train_dir, \"*.png\")))\n",
    "    train_labels = [\n",
    "        int(os.path.basename(m).split(\"_\")[1].split(\".\")[0]) for m in train_paths\n",
    "    ]\n",
    "    epochs = 2\n",
    "\n",
    "# Test data\n",
    "test_dir = os.path.join(BASE_DIR, config[\"data\"][\"test\"][\"test_dir\"], \"masks\")\n",
    "test_paths = sorted(glob(os.path.join(test_dir, \"*.png\")))\n",
    "test_labels = [\n",
    "    int(os.path.basename(m).split(\"_\")[1].split(\".\")[0]) * 100 + 20250\n",
    "    for m in test_paths\n",
    "]\n",
    "\n",
    "# # Create training dataset\n",
    "# input_shape = config[\"CVAE\"][\"input_shape\"]\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "# train_dataset = train_dataset.map(\n",
    "#     lambda mask_path, label: tf.py_function(\n",
    "#         func=load_preprocess_mask,\n",
    "#         inp=[mask_path, label, input_shape[:2], LAST_FRAME],\n",
    "#         Tout=[tf.float32, tf.float32],\n",
    "#     )\n",
    "# )\n",
    "# train_dataset = train_dataset.batch(1)  # batch size 1 for training\n",
    "\n",
    "# # Create testing dataset\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "# test_dataset = test_dataset.map(\n",
    "#     lambda mask_path, label: tf.py_function(\n",
    "#         func=load_preprocess_mask,\n",
    "#         inp=[mask_path, label, input_shape[:2], LAST_FRAME],\n",
    "#         Tout=[tf.float32, tf.float32],\n",
    "#     )\n",
    "# )\n",
    "# test_dataset = test_dataset.batch(1)  # batch size 1 for testing\n",
    "\n",
    "\n",
    "# logger.info(f\" Train samples: {len(train_paths)} | Test samples: {len(test_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAEDataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    A data generator for the Conditional Variational Autoencoder (CVAE) model.\n",
    "\n",
    "    This class generates batches of images and corresponding labels from a given set of data paths and labels.\n",
    "    It shuffles the data at the end of each epoch to ensure that the model sees all data in each epoch.\n",
    "\n",
    "    Attributes:\n",
    "        data_paths: A list of paths to the data files.\n",
    "        labels: A list of corresponding labels for the data files.\n",
    "        batch_size: The number of samples per gradient update.\n",
    "        input_shape: The shape of the input data.\n",
    "        num_frames: The total number of frames in the data.\n",
    "\n",
    "    Methods:\n",
    "        __init__: Initializes the data generator.\n",
    "        __len__: Returns the number of batches in the data.\n",
    "        __getitem__: Returns a batch of images and labels.\n",
    "        on_epoch_end: Shuffles the data at the end of each epoch.\n",
    "        load_and_preprocess_data: Loads and preprocesses a batch of images and labels.\n",
    "        load_preprocess_mask: Loads and preprocesses a single mask image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_paths, labels, batch_size, input_shape, last_frame):\n",
    "        \"\"\"\n",
    "        Initializes the data generator.\n",
    "\n",
    "        Args:\n",
    "            data_paths: A list of paths to the data files.\n",
    "            labels: A list of corresponding labels for the data files.\n",
    "            batch_size: The number of samples per gradient update.\n",
    "            input_shape: The shape of the input data.\n",
    "            last_frame: The total number of frames in the data.\n",
    "        \"\"\"\n",
    "        self.data_paths = data_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.num_frames = last_frame\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of batches in the data.\n",
    "\n",
    "        Returns:\n",
    "            The number of batches in the data.\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.data_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a batch of images and labels.\n",
    "\n",
    "        Args:\n",
    "            index: The index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            A batch of images and labels.\n",
    "        \"\"\"\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = (index + 1) * self.batch_size\n",
    "        batch_data_paths = self.data_paths[start_idx:end_idx]\n",
    "        batch_labels = self.labels[start_idx:end_idx]\n",
    "\n",
    "        batch_images, batch_labels = self.load_and_preprocess_data(\n",
    "            batch_data_paths, batch_labels\n",
    "        )\n",
    "        return [batch_images, batch_labels], batch_images\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Shuffles the data at the end of each epoch.\n",
    "        \"\"\"\n",
    "        indices = np.arange(len(self.data_paths))\n",
    "        np.random.shuffle(indices)\n",
    "        self.data_paths = [self.data_paths[i] for i in indices]\n",
    "        self.labels = [self.labels[i] for i in indices]\n",
    "\n",
    "    def load_and_preprocess_data(self, batch_data_paths, batch_labels):\n",
    "        \"\"\"\n",
    "        Loads and preprocesses a batch of images and labels.\n",
    "\n",
    "        Args:\n",
    "            batch_data_paths: A list of paths to the data files.\n",
    "            batch_labels: A list of corresponding labels for the data files.\n",
    "\n",
    "        Returns:\n",
    "            A batch of images and labels.\n",
    "        \"\"\"\n",
    "        batch_images = []\n",
    "        batch_labels_processed = []\n",
    "        for data_path, label in zip(batch_data_paths, batch_labels):\n",
    "            image, label = self.load_preprocess_mask(\n",
    "                data_path, label, self.input_shape, self.num_frames\n",
    "            )\n",
    "            batch_images.append(image)\n",
    "            batch_labels_processed.append(label)\n",
    "        return np.array(batch_images), np.array(batch_labels_processed)\n",
    "\n",
    "    def load_preprocess_mask(self, mask_path, label, output_dims, last_frame):\n",
    "        \"\"\"\n",
    "        Loads and preprocesses a single mask image.\n",
    "\n",
    "        Args:\n",
    "            mask_path: The path to the mask file.\n",
    "            label: The corresponding label for the mask file.\n",
    "            output_dims: The desired dimensions of the mask.\n",
    "            last_frame: The total number of frames in the data.\n",
    "\n",
    "        Returns:\n",
    "            A preprocessed mask image and its corresponding label.\n",
    "        \"\"\"\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(mask_path):\n",
    "            raise FileNotFoundError(f\"No such file: '{mask_path}'\")\n",
    "\n",
    "        # Read and decode the image\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Resize the image\n",
    "        mask = cv2.resize(mask, output_dims)\n",
    "\n",
    "        # Add channel dimension\n",
    "        mask = np.expand_dims(mask, axis=-1)\n",
    "\n",
    "        # Normalize the mask\n",
    "        mask = (mask / 127.5) - 1\n",
    "\n",
    "        # Normalize the label\n",
    "        label = label / last_frame\n",
    "        label = np.expand_dims(label, axis=-1)\n",
    "\n",
    "        return mask, label\n",
    "\n",
    "\n",
    "input_shape = config[\"CVAE\"][\"input_shape\"]\n",
    "\n",
    "train_data_gen = CVAEDataGenerator(\n",
    "    data_paths=train_paths,\n",
    "    labels=train_labels,\n",
    "    batch_size=1,\n",
    "    input_shape=input_shape[:2],\n",
    "    last_frame=LAST_FRAME,\n",
    ")\n",
    "\n",
    "# Create testing data generator\n",
    "test_data_gen = CVAEDataGenerator(\n",
    "    data_paths=test_paths,\n",
    "    labels=test_labels,\n",
    "    batch_size=1,\n",
    "    input_shape=input_shape[:2],\n",
    "    last_frame=LAST_FRAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-VAE definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv_block(input, filters, f_init=\"he_normal\"):\n",
    "    \"\"\"\n",
    "    Apply two convolutional layers with ReLU activation function.\n",
    "\n",
    "    Args:\n",
    "        input (tensor): Input tensor to the block.\n",
    "        filters (int): Number of filters in the convolutional layers.\n",
    "\n",
    "    Returns:\n",
    "        tensor: Output tensor of the block with ReLU activation.\n",
    "    \"\"\"\n",
    "    x = Conv2DTranspose(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        strides=2,\n",
    "        kernel_initializer=f_init,\n",
    "        data_format=\"channels_last\",\n",
    "        padding=\"same\",\n",
    "    )(input)\n",
    "\n",
    "    x = SeparableConv2D(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        depthwise_initializer=f_init,\n",
    "        pointwise_initializer=f_init,\n",
    "        padding=\"same\",\n",
    "    )(x)\n",
    "    x = Activation(tf.nn.leaky_relu)(x)\n",
    "\n",
    "    x = SeparableConv2D(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        depthwise_initializer=f_init,\n",
    "        pointwise_initializer=f_init,\n",
    "        padding=\"same\",\n",
    "    )(x)\n",
    "    activation = Activation(tf.nn.leaky_relu)(x)\n",
    "\n",
    "    return activation\n",
    "\n",
    "\n",
    "def conv_block(input, filters, f_init=\"he_normal\"):\n",
    "    \"\"\"\n",
    "    Apply two convolutional layers with ReLU activation function.\n",
    "\n",
    "    Args:\n",
    "        input (tensor): Input tensor to the block.\n",
    "        filters (int): Number of filters in the convolutional layers.\n",
    "\n",
    "    Returns:\n",
    "        tensor: Output tensor of the block with ReLU activation.\n",
    "    \"\"\"\n",
    "    x = SeparableConv2D(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        depthwise_initializer=f_init,\n",
    "        pointwise_initializer=f_init,\n",
    "        padding=\"same\",\n",
    "    )(input)\n",
    "    x = Activation(tf.nn.leaky_relu)(x)\n",
    "\n",
    "    x = SeparableConv2D(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        depthwise_initializer=f_init,\n",
    "        pointwise_initializer=f_init,\n",
    "        padding=\"same\",\n",
    "    )(x)\n",
    "    ativ = Activation(tf.nn.leaky_relu)(x)\n",
    "\n",
    "    m_pool = MaxPooling2D(\n",
    "        pool_size=(2, 2), strides=2, data_format=\"channels_last\", padding=\"same\"\n",
    "    )(ativ)\n",
    "\n",
    "    return m_pool\n",
    "\n",
    "\n",
    "def sampler(args):\n",
    "    \"\"\"\n",
    "    Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def mse_kl_loss(y_true, y_pred, beta: float = 1.0):\n",
    "    \"\"\"Calculate loss = reconstruction loss + KL loss for each data in minibatch\"\"\"\n",
    "    # E[log P(X|z)]\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    reconstruction = tf.reduce_mean(squared_difference, axis=-1)\n",
    "    # D_KL(Q(z|X) || P(z|X)); calculate in closed from as both dist. are Gaussian\n",
    "    kl_divergence = 0.5 * tf.reduce_sum(\n",
    "        tf.exp(z_log_var) + tf.square(z_mean) - 1.0 - z_log_var, axis=-1\n",
    "    )\n",
    "    return reconstruction + beta * kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cvae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 512, 512, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        [(None, 64),                 3357281   ['input_2[0][0]']             \n",
      "                              (None, 64),                 6                                       \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " label (InputLayer)          [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, 512, 512, 1)          1749779   ['encoder[0][2]',             \n",
      "                                                          3          'label[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 51070609 (194.82 MB)\n",
      "Trainable params: 51070609 (194.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "H, W, C = config[\"CVAE\"][\"input_shape\"]\n",
    "\n",
    "# --------\n",
    "# Encoder\n",
    "# --------\n",
    "\n",
    "encoder_inputs = Input(shape=(H, W, C))\n",
    "# Reshape input to 2D image\n",
    "\n",
    "x = conv_block(\n",
    "    encoder_inputs, config[\"CVAE\"][\"ref_filters\"] * 2, config[\"CVAE\"][\"w_init\"]\n",
    ")\n",
    "x = conv_block(x, config[\"CVAE\"][\"ref_filters\"] * 1, config[\"CVAE\"][\"w_init\"])\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation=\"leaky_relu\")(x)\n",
    "\n",
    "# VAE specific layers for mean and log variance\n",
    "z_mean = Dense(config[\"CVAE\"][\"latent_dim\"], activation=\"leaky_relu\", name=\"z_mean\")(x)\n",
    "z_log_var = Dense(\n",
    "    config[\"CVAE\"][\"latent_dim\"], activation=\"leaky_relu\", name=\"z_log_var\"\n",
    ")(x)\n",
    "\n",
    "# Sampling layer to sample z from the latent space\n",
    "z = Lambda(sampler, output_shape=(config[\"CVAE\"][\"latent_dim\"],), name=\"z\")(\n",
    "    [z_mean, z_log_var]\n",
    ")\n",
    "\n",
    "# Instantiate encoder model\n",
    "encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "# --------\n",
    "# Decoder\n",
    "# --------\n",
    "\n",
    "latent_inputs = Input(shape=(config[\"CVAE\"][\"latent_dim\"],), name=\"z_sampling\")\n",
    "label_size = 1 # one tf.float32 label\n",
    "label_inputs = Input(shape=(label_size,), name=\"label\")\n",
    "decoder_inputs = Concatenate()([latent_inputs, label_inputs])\n",
    "x = Dense(64 * 64 * 64, activation=\"leaky_relu\")(decoder_inputs)\n",
    "x = Reshape((128, 128, 16))(x)\n",
    "x = deconv_block(x, config[\"CVAE\"][\"ref_filters\"] * 2, config[\"CVAE\"][\"w_init\"])\n",
    "x = deconv_block(x, config[\"CVAE\"][\"ref_filters\"] * 4, config[\"CVAE\"][\"w_init\"])\n",
    "decoder_output = Conv2DTranspose(1, 3, activation=\"tanh\", padding=\"same\")(x)\n",
    "\n",
    "decoder = Model([latent_inputs, label_inputs], decoder_output, name=\"decoder\")\n",
    "\n",
    "# -----------------\n",
    "# Conditional VAE\n",
    "# -----------------\n",
    "\n",
    "outputs = decoder([encoder(encoder_inputs)[2], label_inputs])\n",
    "cvae = Model([encoder_inputs, label_inputs], outputs, name=\"cvae\")\n",
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"loss\", factor=0.5, mode=\"min\", patience=30, verbose=1, min_lr=1e-8\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    min_delta=0,\n",
    "    patience=40,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "checkpoint_dir = os.path.join(BASE_DIR, config[\"data\"][\"checkpoint_dir\"])\n",
    "if MODE == \"extrapol\":\n",
    "    checkpoint_filepath = os.path.join(\n",
    "        checkpoint_dir, f\"cvae_{DATA}_{MODE}_{PERCENTAGE}.h5\"\n",
    "    )\n",
    "elif MODE == \"interpol\":\n",
    "    checkpoint_filepath = os.path.join(checkpoint_dir, f\"cvae_{DATA}_{MODE}.h5\")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_best_only=True,\n",
    "    mode=\"auto\",\n",
    "    verbose=1,\n",
    "    monitor=\"loss\",\n",
    ")\n",
    "\n",
    "cvae.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate=config[\"CVAE\"][\"learning_rate\"]\n",
    "    ),\n",
    "    loss=mse_kl_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "22530/22530 [==============================] - ETA: 0s - batch: 11264.5000 - size: 1.0000 - loss: 0.0998"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagociic/miniconda3/envs/cvae/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    }
   ],
   "source": [
    "cvae.optimizer.lr = config[\"CVAE\"][\"learning_rate\"]\n",
    "\n",
    "# Fit the model\n",
    "history = cvae.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=len(train_data_gen),\n",
    "    epochs=epochs,\n",
    "    validation_data=test_data_gen,\n",
    "    validation_steps=len(test_data_gen),\n",
    "    callbacks=[reduce_lr, checkpoint, early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot and save learning curves\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m MODE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextrapol\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     save_history(\n\u001b[0;32m----> 4\u001b[0m         \u001b[43mhistory\u001b[49m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPERCENTAGE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# plot_learning_curves(history, log_scale=True,plt_title=f\"CVAE {DATA} {MODE} {PERCENTAGE}%\", save_fig = True)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     plot_learning_curves(\n\u001b[1;32m      8\u001b[0m         history, log_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, plt_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCVAE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPERCENTAGE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# plot and save learning curves\n",
    "if MODE == \"extrapol\":\n",
    "    save_history(\n",
    "        history, os.path.join(checkpoint_dir, f\"history_{DATA}_{MODE}_{PERCENTAGE}.csv\")\n",
    "    )\n",
    "    # plot_learning_curves(history, log_scale=True,plt_title=f\"CVAE {DATA} {MODE} {PERCENTAGE}%\", save_fig = True)\n",
    "    plot_learning_curves(\n",
    "        history, log_scale=True, plt_title=f\"CVAE {DATA} {MODE} {PERCENTAGE}%\"\n",
    "    )\n",
    "elif MODE == \"interpol\":\n",
    "    save_history(history, os.path.join(checkpoint_dir, f\"history_{DATA}_{MODE}.csv\"))\n",
    "    # plot_learning_curves(history, log_scale=True,plt_title=f\"CVAE {DATA} {MODE}\", save_fig = True)\n",
    "    plot_learning_curves(history, log_scale=True, plt_title=f\"CVAE {DATA} {MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "cvae.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frames(\n",
    "    decoder, output_dir: str, total_frames: int = 22500, resize_original: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates and saves the frames from a trained decoder.\n",
    "\n",
    "    Parameters:\n",
    "        decoder (keras.Model): The trained decoder.\n",
    "        output_dir (str): The path to the output directory.\n",
    "        total_frames (int): The total number of frames to generate.\n",
    "        resize_original (bool): Whether to resize the frames to the original dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    start_total_time = time.time()\n",
    "\n",
    "    frames_num = np.arange(1, total_frames + 1, 1)\n",
    "\n",
    "    for i in range(total_frames):\n",
    "        frame_num = frames_num[i]\n",
    "\n",
    "        # Sample from the latent space\n",
    "        z_sample = np.full((1, config[\"CVAE\"][\"latent_dim\"]), 0.5)\n",
    "\n",
    "        # Generate the frame\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            reconst = decoder.predict([z_sample, frame_to_label(frame_num)])\n",
    "            reconst_time = (time.time() - start_time) * 1000\n",
    "            reconst = np.squeeze(reconst, axis=0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating frame {frame_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if resize_original:\n",
    "            start_time = time.time()\n",
    "            reconst = tf.image.resize(\n",
    "                images=reconst, size=config[\"data\"][\"original_vid_dims\"]\n",
    "            )\n",
    "            resize_time = (time.time() - start_time) * 1000\n",
    "        else:\n",
    "            resize_time = 0.0  # Not resizing\n",
    "\n",
    "        # Binarize the reconstructed image with OpenCV\n",
    "        start_time = time.time()\n",
    "        _, thresh_img = cv2.threshold(\n",
    "            reconst, config[\"CVAE\"][\"threshold\"], 255, cv2.THRESH_BINARY\n",
    "        )\n",
    "        threshold_time = (time.time() - start_time) * 1000\n",
    "\n",
    "        # Save the thresholded image as png in grayscale\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            cv2.imwrite(\n",
    "                os.path.join(output_dir, f\"frame_{frame_num:06d}.png\"), thresh_img\n",
    "            )\n",
    "            save_time = (time.time() - start_time) * 1000\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving frame {frame_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Print progress with time information\n",
    "        print(\n",
    "            f\"Generated frame {i+1} of {total_frames} | \"\n",
    "            f\"Reconst: {reconst_time:.2f}ms | \"\n",
    "            f\"Resize: {resize_time:.2f}ms | \"\n",
    "            f\"Threshold: {threshold_time:.2f}ms | \"\n",
    "            f\"Save: {save_time:.2f}ms | \"\n",
    "            f\"Elapsed Time: {time.time() - start_total_time:.2f}s  \",\n",
    "            end=\"\\r\",\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagociic/miniconda3/envs/cvae/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated frame 22500 of 22500 | Reconst: 24.90ms | Resize: 0.00ms | Threshold: 0.11ms | Save: 0.73ms | Elapsed Time: 588.96s  \n"
     ]
    }
   ],
   "source": [
    "output_png_dir = os.path.join(output_dir, \"PNG\")\n",
    "generate_frames(decoder, output_png_dir, total_frames=max_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Creating image list...                          \n",
      "INFO - Writing frames to file 1/22500\n",
      "INFO - Writing frames to file 1001/22500\n",
      "INFO - Writing frames to file 2001/22500\n",
      "INFO - Writing frames to file 3001/22500\n",
      "INFO - Writing frames to file 4001/22500\n",
      "INFO - Writing frames to file 5001/22500\n",
      "INFO - Writing frames to file 6001/22500\n",
      "INFO - Writing frames to file 7001/22500\n",
      "INFO - Writing frames to file 8001/22500\n",
      "INFO - Writing frames to file 9001/22500\n",
      "INFO - Writing frames to file 10001/22500\n",
      "INFO - Writing frames to file 11001/22500\n",
      "INFO - Writing frames to file 12001/22500\n",
      "INFO - Writing frames to file 13001/22500\n",
      "INFO - Writing frames to file 14001/22500\n",
      "INFO - Writing frames to file 15001/22500\n",
      "INFO - Writing frames to file 16001/22500\n",
      "INFO - Writing frames to file 17001/22500\n",
      "INFO - Writing frames to file 18001/22500\n",
      "INFO - Writing frames to file 19001/22500\n",
      "INFO - Writing frames to file 20001/22500\n",
      "INFO - Writing frames to file 21001/22500\n",
      "INFO - Writing frames to file 22001/22500\n",
      "INFO - Saved video to /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/outputs/CVAE/extrapol/70/full/video_full_extrapol_70.mp4\n"
     ]
    }
   ],
   "source": [
    "# generate video from the generated frames\n",
    "if MODE == \"extrapol\":\n",
    "    file_name = f\"video_{DATA}_{MODE}_{PERCENTAGE}\"\n",
    "    title = f\"CVAE: {MODE}ation - {DATA}, {PERCENTAGE}, {config['CVAE']['epochs']} epochs, 10x speed\"\n",
    "elif MODE == \"interpol\":\n",
    "    file_name = f\"video_{DATA}_{MODE}\"\n",
    "    title = f\"CVAE: {MODE}ation - {DATA}, {config['CVAE']['epochs']} epochs, 10x speed\"\n",
    "\n",
    "frames_to_video(\n",
    "    img_list_dir=os.path.join(output_dir, \"PNG\"),\n",
    "    output_dir=output_dir,\n",
    "    output_resolution=config[\"data\"][\"original_vid_dims\"],\n",
    "    title=title,\n",
    "    f_ps=250,  # 10x speed\n",
    "    file_name=file_name,\n",
    "    frame_num_text=True,\n",
    "    font_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagociic/miniconda3/envs/cvae/lib/python3.10/site-packages/rasterio/features.py:126: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  for s, v in _shapes(source, mask, connectivity, transform):\n",
      "/home/tiagociic/miniconda3/envs/cvae/lib/python3.10/site-packages/rasterio/features.py:126: NotGeoreferencedWarning: The given matrix is equal to Affine.identity or its flipped counterpart. GDAL may ignore this matrix and save no geotransform without raising an error. This behavior is somewhat driver-specific.\n",
      "  for s, v in _shapes(source, mask, connectivity, transform):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m msks_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_png_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert the masks to polygons and save them as a WKT file\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m polygons \u001b[38;5;241m=\u001b[39m \u001b[43mmasks_to_polygons_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsks_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_vid_dims\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWKT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.wkt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m,\n",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m, in \u001b[0;36mmasks_to_polygons_v2\u001b[0;34m(msks_paths, out_dim, save_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     35\u001b[0m counter \u001b[38;5;241m=\u001b[39m Value(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     38\u001b[0m     futures \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(resize_image, img_path, counter, out_dim): img_path \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m msks_paths}\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Collect results in order\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvae/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvae/lib/python3.10/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvae/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/cvae/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List of generated frames paths\n",
    "msks_paths = sorted(glob(os.path.join(output_png_dir, \"*.png\")))\n",
    "\n",
    "# Convert the masks to polygons and save them as a WKT file\n",
    "polygons = (\n",
    "    masks_to_polygons(\n",
    "        msks_paths,\n",
    "        out_dim=tuple(config[\"data\"][\"original_vid_dims\"]),\n",
    "        save_path=os.path.join(\n",
    "            BASE_DIR, MODEL, MODE, DATA, \"WKT\", f\"{MODE}_{DATA}.wkt\"\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Value\n",
    "from multiprocessing import cpu_count\n",
    "from time import sleep\n",
    "from os import cpu_count\n",
    "from time import time\n",
    "from functools import partial\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from rasterio.features import shapes\n",
    "from rasterio import Affine\n",
    "from shapely.geometry import shape, MultiPolygon\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def resize_image(img_path, counter, out_dim=(512, 512)):\n",
    "    try:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        h, w = img.shape\n",
    "        if (w, h) != out_dim:\n",
    "            img = cv2.resize(img, out_dim, interpolation=cv2.INTER_CUBIC)\n",
    "        return mask_to_poly(img)\n",
    "    except Exception as e:\n",
    "        logger.ERROR(f\"Error processing {img_path}: {e}\")\n",
    "    finally:\n",
    "        with counter.get_lock():\n",
    "            counter.value += 1\n",
    "\n",
    "\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "\n",
    "def masks_to_polygons_v2(\n",
    "    msks_paths: list, out_dim: tuple = (512, 512), save_path: str = None\n",
    ") -> list:\n",
    "    start_time = time.time()\n",
    "    counter = Value(\"i\", 0)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = {\n",
    "            executor.submit(resize_image, img_path, counter, out_dim): img_path\n",
    "            for img_path in msks_paths\n",
    "        }\n",
    "\n",
    "    # Collect results in order\n",
    "    pol_list = []\n",
    "    for future in as_completed(futures):\n",
    "        pol_list.append(future.result())\n",
    "\n",
    "        # Print progress\n",
    "        with counter.get_lock():\n",
    "            counter.value += 1\n",
    "            print(f\"Processed {counter.value} masks\", end=\"\\r\", flush=True)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nProcessed {len(msks_paths)} masks | Time elapsed: {elapsed_time:.2f}s\")\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if save_path:\n",
    "        save_polygons_to_wkt(pol_list, save_path)\n",
    "        logger.info(f\"Saved polygons to {save_path}\")\n",
    "\n",
    "    return pol_list\n",
    "\n",
    "\n",
    "def save_polygons_to_wkt(polygon_list: list, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save a list of shapely polygons to a WKT format file.\n",
    "\n",
    "    Parameters:\n",
    "    polygon_list (list): List of shapely polygons.\n",
    "    file_path (str): Path to the output file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for polygon in polygon_list:\n",
    "            f.write(polygon.wkt + \"\\n\")\n",
    "\n",
    "\n",
    "def mask_to_poly(mask_img: np.ndarray) -> MultiPolygon:\n",
    "    \"\"\"\n",
    "    Converts a segmentation mask to a shapely multipolygon.\n",
    "\n",
    "    Parameters:\n",
    "    mask_img (numpy.ndarray): The segmentation mask.\n",
    "\n",
    "    Returns:\n",
    "    shapely.geometry.MultiPolygon: The shapely multipolygon.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_polygons = list()\n",
    "\n",
    "    for shp, _ in shapes(\n",
    "        source=mask_img.astype(np.uint8),\n",
    "        mask=(mask_img > 0),\n",
    "        transform=Affine(1.0, 0, 0, 0, 1.0, 0),\n",
    "    ):\n",
    "        all_polygons.append(shape(shp))\n",
    "\n",
    "    all_polygons = MultiPolygon(all_polygons)\n",
    "\n",
    "    if not all_polygons.is_valid:\n",
    "        all_polygons = all_polygons.buffer(0)\n",
    "        if all_polygons.geom_type == \"Polygon\":\n",
    "            all_polygons = MultiPolygon([all_polygons])\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logger.debug(f\"mask_to_poly elapsed time: {elapsed_time:.2f} seconds  \", end=\"\\r\")\n",
    "\n",
    "    return all_polygons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
