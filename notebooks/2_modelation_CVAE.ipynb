{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Tensorflow Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Lambda, Input, Dense\n",
    "from tensorflow.keras.layers import MaxPooling2D, Flatten, Reshape, Concatenate\n",
    "from tensorflow.keras.layers import SeparableConv2D, Conv2DTranspose\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "disable_eager_execution()\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Local Module Imports\n",
    "sys.path.append(\"../src\")  # adds source code directory\n",
    "from utils import frame_to_label\n",
    "from utils import frames_to_video, save_history\n",
    "from visualization import plot_learning_curves\n",
    "from polygon_handle import masks_to_polygons\n",
    "from log_setup import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "DATA: \"full\" (full dataset), \"sampled\" (distance sampled dataset) \n",
    "        or \"unet\" (unet generated dataset)\n",
    "MODE: \"interpol\" (interpolation) or \"extrapol\" (extrapolation)\n",
    "MODEL: \"CVAE\"\n",
    "PERCENTAGE: percentage of training data to be used for training\n",
    "LAST_FRAME: last frame number of the video\n",
    "\"\"\"\n",
    "\n",
    "DATA = \"unet\"\n",
    "MODE = \"extrapol\"\n",
    "MODEL = \"CVAE\"\n",
    "PERCENTAGE = 50\n",
    "LAST_FRAME = 22500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Data: unet, Mode: extrapol, Model: CVAE Percentage: 50%,\n",
      "Output directory: /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/outputs/CVAE/extrapol/50/unet\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "dataset_dir = os.path.join(BASE_DIR, \"dataset\")\n",
    "data_dir = os.path.join(BASE_DIR, \"data\")\n",
    "config_file = os.path.join(BASE_DIR, \"config.yml\")\n",
    "\n",
    "# Output PNG directory\n",
    "if MODE == \"extrapol\":\n",
    "    output_dir = os.path.join(BASE_DIR, \"outputs\", \"CVAE\", MODE, str(PERCENTAGE), DATA)\n",
    "    logger.info(\n",
    "        f\"Data: {DATA}, Mode: {MODE}, Model: {MODEL} Percentage: {PERCENTAGE}%,\\nOutput directory: {output_dir}\"\n",
    "    )\n",
    "elif MODE == \"interpol\":\n",
    "    output_dir = os.path.join(BASE_DIR, \"outputs\", \"CVAE\", MODE, DATA)\n",
    "    logger.info(\n",
    "        f\"\\nData: {DATA}, Mode: {MODE}, Model: {MODEL}\\nOutput directory: {output_dir}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - No. train. samples: 11265 out of 22500 (50%) | No. test samples: 23\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "if DATA == \"full\":\n",
    "    train_dir = os.path.join(BASE_DIR, config[\"data\"][\"full\"][\"train_dir\"], \"masks\")\n",
    "    # sort the paths\n",
    "    train_paths = sorted(glob(os.path.join(train_dir, \"*.png\")))\n",
    "    # extract labels from the paths\n",
    "    train_labels = [\n",
    "        int(os.path.basename(m).split(\"_\")[1].split(\".\")[0]) * 100 for m in train_paths\n",
    "    ]\n",
    "    epochs = config[\"CVAE\"][\"epochs\"]\n",
    "\n",
    "elif DATA == \"sampled\":\n",
    "    sampled_masks_txt_path = os.path.join(BASE_DIR, config[\"data\"][\"wkt\"][\"sampled_masks_txt\"])\n",
    "    with open(sampled_masks_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        polygons = f.readlines()\n",
    "        # extract indexes\n",
    "    indexes = [int(polygon.split(\",\")[0]) for polygon in polygons]\n",
    "    train_dir = os.path.join(BASE_DIR, config[\"data\"][\"sampled\"][\"train_dir\"], \"masks\")\n",
    "    train_paths = sorted(glob(os.path.join(train_dir, \"*.png\")))\n",
    "    train_labels = [100 * i for i in indexes]\n",
    "    epochs = config[\"CVAE\"][\"epochs\"]\n",
    "\n",
    "elif DATA == \"unet\":\n",
    "    train_dir = os.path.join(BASE_DIR, config[\"data\"][\"unet\"][\"train_dir\"], \"masks\")\n",
    "    train_paths = sorted(glob(os.path.join(train_dir, \"*.png\")))\n",
    "    train_labels = [\n",
    "        int(os.path.basename(m).split(\"_\")[1].split(\".\")[0]) for m in train_paths\n",
    "    ]\n",
    "    epochs = 2\n",
    "\n",
    "\n",
    "# Test data\n",
    "test_dir = os.path.join(BASE_DIR, config[\"data\"][\"test\"][\"test_dir\"], \"masks\")\n",
    "test_paths = sorted(glob(os.path.join(test_dir, \"*.png\")))\n",
    "test_labels = [\n",
    "    int(os.path.basename(m).split(\"_\")[1].split(\".\")[0]) * 100 + 20250\n",
    "    for m in test_paths\n",
    "]\n",
    "\n",
    "if MODE == \"extrapol\":\n",
    "    # Truncate the training data\n",
    "    train_paths = train_paths[: int(len(train_paths) * PERCENTAGE / 100)]\n",
    "    train_labels = train_labels[: int(len(train_labels) * PERCENTAGE / 100)]\n",
    "    logger.info(f\"No. train. samples: {len(train_paths)} out of {LAST_FRAME} ({PERCENTAGE}%) | No. test samples: {len(test_paths)}\")\n",
    "elif MODE == \"interpol\":\n",
    "    logger.info(f\"No. train. samples: {len(train_paths)} out of {LAST_FRAME} | No. test samples: {len(test_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 'sampled_masks_txt' not found in the config data.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sampled_masks_txt_path = os.path.join(BASE_DIR, config[\"data\"][\"sampled_masks_txt\"])\n",
    "except KeyError:\n",
    "    print(\"Key 'sampled_masks_txt' not found in the config data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAEDataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    A data generator for the Conditional Variational Autoencoder (CVAE) model.\n",
    "\n",
    "    This class generates batches of images and corresponding labels from a given set of data paths and labels.\n",
    "    It shuffles the data at the end of each epoch to ensure that the model sees all data in each epoch.\n",
    "\n",
    "    Attributes:\n",
    "        data_paths: A list of paths to the data files.\n",
    "        labels: A list of corresponding labels for the data files.\n",
    "        batch_size: The number of samples per gradient update.\n",
    "        input_shape: The shape of the input data.\n",
    "        num_frames: The total number of frames in the data.\n",
    "\n",
    "    Methods:\n",
    "        __init__: Initializes the data generator.\n",
    "        __len__: Returns the number of batches in the data.\n",
    "        __getitem__: Returns a batch of images and labels.\n",
    "        on_epoch_end: Shuffles the data at the end of each epoch.\n",
    "        load_and_preprocess_data: Loads and preprocesses a batch of images and labels.\n",
    "        load_preprocess_mask: Loads and preprocesses a single mask image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_paths, labels, batch_size, input_shape, last_frame):\n",
    "        \"\"\"\n",
    "        Initializes the data generator.\n",
    "\n",
    "        Args:\n",
    "            data_paths: A list of paths to the data files.\n",
    "            labels: A list of corresponding labels for the data files.\n",
    "            batch_size: The number of samples per gradient update.\n",
    "            input_shape: The shape of the input data.\n",
    "            last_frame: The total number of frames in the data.\n",
    "        \"\"\"\n",
    "        self.data_paths = data_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.num_frames = last_frame\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of batches in the data.\n",
    "\n",
    "        Returns:\n",
    "            The number of batches in the data.\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.data_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a batch of images and labels.\n",
    "\n",
    "        Args:\n",
    "            index: The index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            A batch of images and labels.\n",
    "        \"\"\"\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = (index + 1) * self.batch_size\n",
    "        batch_data_paths = self.data_paths[start_idx:end_idx]\n",
    "        batch_labels = self.labels[start_idx:end_idx]\n",
    "\n",
    "        batch_images, batch_labels = self.load_and_preprocess_data(\n",
    "            batch_data_paths, batch_labels\n",
    "        )\n",
    "        return [batch_images, batch_labels], batch_images\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Shuffles the data at the end of each epoch.\n",
    "        \"\"\"\n",
    "        indices = np.arange(len(self.data_paths))\n",
    "        np.random.shuffle(indices)\n",
    "        self.data_paths = [self.data_paths[i] for i in indices]\n",
    "        self.labels = [self.labels[i] for i in indices]\n",
    "\n",
    "    def load_and_preprocess_data(self, batch_data_paths, batch_labels):\n",
    "        \"\"\"\n",
    "        Loads and preprocesses a batch of images and labels.\n",
    "\n",
    "        Args:\n",
    "            batch_data_paths: A list of paths to the data files.\n",
    "            batch_labels: A list of corresponding labels for the data files.\n",
    "\n",
    "        Returns:\n",
    "            A batch of images and labels.\n",
    "        \"\"\"\n",
    "        batch_images = []\n",
    "        batch_labels_processed = []\n",
    "        for data_path, label in zip(batch_data_paths, batch_labels):\n",
    "            image, label = self.load_preprocess_mask(\n",
    "                data_path, label, self.input_shape, self.num_frames\n",
    "            )\n",
    "            batch_images.append(image)\n",
    "            batch_labels_processed.append(label)\n",
    "        return np.array(batch_images), np.array(batch_labels_processed)\n",
    "\n",
    "    def load_preprocess_mask(self, mask_path, label, output_dims, last_frame):\n",
    "        \"\"\"\n",
    "        Loads and preprocesses a single mask image.\n",
    "\n",
    "        Args:\n",
    "            mask_path: The path to the mask file.\n",
    "            label: The corresponding label for the mask file.\n",
    "            output_dims: The desired dimensions of the mask.\n",
    "            last_frame: The total number of frames in the data.\n",
    "\n",
    "        Returns:\n",
    "            A preprocessed mask image and its corresponding label.\n",
    "        \"\"\"\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(mask_path):\n",
    "            raise FileNotFoundError(f\"No such file: '{mask_path}'\")\n",
    "\n",
    "        # Read and decode the image\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Resize the image\n",
    "        mask = cv2.resize(mask, output_dims)\n",
    "\n",
    "        # Add channel dimension\n",
    "        mask = np.expand_dims(mask, axis=-1)\n",
    "\n",
    "        # Normalize the mask\n",
    "        mask = (mask / 127.5) - 1\n",
    "\n",
    "        # Normalize the label\n",
    "        label = label / last_frame\n",
    "        label = np.expand_dims(label, axis=-1)\n",
    "\n",
    "        return mask, label\n",
    "\n",
    "\n",
    "input_shape = config[\"CVAE\"][\"input_shape\"]\n",
    "\n",
    "# Create training data generator\n",
    "train_data_gen = CVAEDataGenerator(\n",
    "    data_paths=train_paths,\n",
    "    labels=train_labels,\n",
    "    batch_size=1,\n",
    "    input_shape=input_shape[:2],\n",
    "    last_frame=LAST_FRAME,\n",
    ")\n",
    "\n",
    "# Create testing data generator\n",
    "test_data_gen = CVAEDataGenerator(\n",
    "    data_paths=test_paths,\n",
    "    labels=test_labels,\n",
    "    batch_size=1,\n",
    "    input_shape=input_shape[:2],\n",
    "    last_frame=LAST_FRAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-VAE definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv_block(input, filters, f_init=\"he_normal\"):\n",
    "    \"\"\"\n",
    "    Apply two convolutional layers with ReLU activation function.\n",
    "\n",
    "    Args:\n",
    "        input (tensor): Input tensor to the block.\n",
    "        filters (int): Number of filters in the convolutional layers.\n",
    "\n",
    "    Returns:\n",
    "        tensor: Output tensor of the block with ReLU activation.\n",
    "    \"\"\"\n",
    "    x = Conv2DTranspose(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        strides=2,\n",
    "        kernel_initializer=f_init,\n",
    "        data_format=\"channels_last\",\n",
    "        padding=\"same\",\n",
    "    )(input)\n",
    "\n",
    "    x = SeparableConv2D(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        depthwise_initializer=f_init,\n",
    "        pointwise_initializer=f_init,\n",
    "        padding=\"same\",\n",
    "    )(x)\n",
    "    x = Activation(tf.nn.leaky_relu)(x)\n",
    "\n",
    "    x = SeparableConv2D(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        depthwise_initializer=f_init,\n",
    "        pointwise_initializer=f_init,\n",
    "        padding=\"same\",\n",
    "    )(x)\n",
    "    activation = Activation(tf.nn.leaky_relu)(x)\n",
    "\n",
    "    return activation\n",
    "\n",
    "\n",
    "def conv_block(input, filters, f_init=\"he_normal\"):\n",
    "    \"\"\"\n",
    "    Apply two convolutional layers with ReLU activation function.\n",
    "\n",
    "    Args:\n",
    "        input (tensor): Input tensor to the block.\n",
    "        filters (int): Number of filters in the convolutional layers.\n",
    "\n",
    "    Returns:\n",
    "        tensor: Output tensor of the block with ReLU activation.\n",
    "    \"\"\"\n",
    "    x = SeparableConv2D(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        depthwise_initializer=f_init,\n",
    "        pointwise_initializer=f_init,\n",
    "        padding=\"same\",\n",
    "    )(input)\n",
    "    x = Activation(tf.nn.leaky_relu)(x)\n",
    "\n",
    "    x = SeparableConv2D(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        depthwise_initializer=f_init,\n",
    "        pointwise_initializer=f_init,\n",
    "        padding=\"same\",\n",
    "    )(x)\n",
    "    ativ = Activation(tf.nn.leaky_relu)(x)\n",
    "\n",
    "    m_pool = MaxPooling2D(\n",
    "        pool_size=(2, 2), strides=2, data_format=\"channels_last\", padding=\"same\"\n",
    "    )(ativ)\n",
    "\n",
    "    return m_pool\n",
    "\n",
    "\n",
    "def sampler(args):\n",
    "    \"\"\"\n",
    "    Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def mse_kl_loss(y_true, y_pred, beta: float = 1.0):\n",
    "    \"\"\"Calculate loss = reconstruction loss + KL loss for each data in minibatch\"\"\"\n",
    "    # E[log P(X|z)]\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    reconstruction = tf.reduce_mean(squared_difference, axis=-1)\n",
    "    # D_KL(Q(z|X) || P(z|X)); calculate in closed from as both dist. are Gaussian\n",
    "    kl_divergence = 0.5 * tf.reduce_sum(\n",
    "        tf.exp(z_log_var) + tf.square(z_mean) - 1.0 - z_log_var, axis=-1\n",
    "    )\n",
    "    return reconstruction + beta * kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cvae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 512, 512, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        [(None, 64),                 3357281   ['input_2[0][0]']             \n",
      "                              (None, 64),                 6                                       \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " label (InputLayer)          [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, 512, 512, 1)          1749779   ['encoder[0][2]',             \n",
      "                                                          3          'label[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 51070609 (194.82 MB)\n",
      "Trainable params: 51070609 (194.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "H, W, C = config[\"CVAE\"][\"input_shape\"]\n",
    "\n",
    "# --------\n",
    "# Encoder\n",
    "# --------\n",
    "\n",
    "encoder_inputs = Input(shape=(H, W, C))\n",
    "# Reshape input to 2D image\n",
    "\n",
    "x = conv_block(\n",
    "    encoder_inputs, config[\"CVAE\"][\"ref_filters\"] * 2, config[\"CVAE\"][\"w_init\"]\n",
    ")\n",
    "x = conv_block(x, config[\"CVAE\"][\"ref_filters\"] * 1, config[\"CVAE\"][\"w_init\"])\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation=\"leaky_relu\")(x)\n",
    "\n",
    "# VAE specific layers for mean and log variance\n",
    "z_mean = Dense(config[\"CVAE\"][\"latent_dim\"], activation=\"leaky_relu\", name=\"z_mean\")(x)\n",
    "z_log_var = Dense(\n",
    "    config[\"CVAE\"][\"latent_dim\"], activation=\"leaky_relu\", name=\"z_log_var\"\n",
    ")(x)\n",
    "\n",
    "# Sampling layer to sample z from the latent space\n",
    "z = Lambda(sampler, output_shape=(config[\"CVAE\"][\"latent_dim\"],), name=\"z\")(\n",
    "    [z_mean, z_log_var]\n",
    ")\n",
    "\n",
    "# Instantiate encoder model\n",
    "encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "# --------\n",
    "# Decoder\n",
    "# --------\n",
    "\n",
    "latent_inputs = Input(shape=(config[\"CVAE\"][\"latent_dim\"],), name=\"z_sampling\")\n",
    "label_size = 1  # one tf.float32 label\n",
    "label_inputs = Input(shape=(label_size,), name=\"label\")\n",
    "decoder_inputs = Concatenate()([latent_inputs, label_inputs])\n",
    "x = Dense(64 * 64 * 64, activation=\"leaky_relu\")(decoder_inputs)\n",
    "x = Reshape((128, 128, 16))(x)\n",
    "x = deconv_block(x, config[\"CVAE\"][\"ref_filters\"] * 2, config[\"CVAE\"][\"w_init\"])\n",
    "x = deconv_block(x, config[\"CVAE\"][\"ref_filters\"] * 4, config[\"CVAE\"][\"w_init\"])\n",
    "decoder_output = Conv2DTranspose(1, 3, activation=\"tanh\", padding=\"same\")(x)\n",
    "\n",
    "decoder = Model([latent_inputs, label_inputs], decoder_output, name=\"decoder\")\n",
    "\n",
    "# -----------------\n",
    "# Conditional VAE\n",
    "# -----------------\n",
    "\n",
    "outputs = decoder([encoder(encoder_inputs)[2], label_inputs])\n",
    "cvae = Model([encoder_inputs, label_inputs], outputs, name=\"cvae\")\n",
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceLROnPlateauSteps(tf.keras.callbacks.ReduceLROnPlateau):\n",
    "    def __init__(\n",
    "        self, monitor=\"val_loss\", factor=0.5, patience=500, min_lr=1e-8, **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            monitor=monitor, factor=factor, patience=patience, min_lr=min_lr, **kwargs\n",
    "        )\n",
    "        self.wait = 0\n",
    "        self.best = 0\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        current = self.get_monitor_value(logs)\n",
    "        if current < self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.wait = 0\n",
    "                new_lr = self.model.optimizer.learning_rate * self.factor\n",
    "                new_lr = tf.keras.backend.get_value(new_lr)\n",
    "                self.model.optimizer.learning_rate = new_lr\n",
    "                print(\"Reducing learning rate to %s.\" % (new_lr,))\n",
    "\n",
    "    def get_monitor_value(self, logs):\n",
    "        logs = logs or {}\n",
    "        monitor_value = logs.get(self.monitor)\n",
    "        if monitor_value is None:\n",
    "            logger.warning(\n",
    "                \"Learning rate reduction on plateau conditioned on metric `%s` \"\n",
    "                \"which is not available. Available metrics are: %s\"\n",
    "                % (self.monitor, \", \".join(list(logs.keys()))),\n",
    "                RuntimeWarning,\n",
    "            )\n",
    "        return monitor_value\n",
    "\n",
    "\n",
    "class EarlyStoppingSteps(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, monitor=\"val_loss\", min_delta=0, patience=500, **kwargs):\n",
    "        super().__init__(\n",
    "            monitor=monitor, min_delta=min_delta, patience=patience, **kwargs\n",
    "        )\n",
    "        self.wait = 0\n",
    "        self.best = 0\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        \"\"\"\n",
    "        At the end of each batch, check if the monitored quantity has improved.\n",
    "        If number of batches since the last improvement is more than the patience,\n",
    "        stop training.\n",
    "        \"\"\"\n",
    "        current = self.get_monitor_value(logs)\n",
    "        if current < self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.wait = 0\n",
    "                self.stopped_epoch = self.model.history.epoch[-1]\n",
    "                self.model.stop_training = True\n",
    "                print(\"Early stopping\")\n",
    "\n",
    "    def get_monitor_value(self, logs):\n",
    "        logs = logs or {}\n",
    "        monitor_value = logs.get(self.monitor)\n",
    "        if monitor_value is None:\n",
    "            logger.warning(\n",
    "                \"Early stopping conditioned on metric `%s` \"\n",
    "                \"which is not available. Available metrics are: %s\"\n",
    "                % (self.monitor, \", \".join(list(logs.keys()))),\n",
    "                RuntimeWarning,\n",
    "            )\n",
    "        return monitor_value\n",
    "\n",
    "\n",
    "class ModelCheckpointSteps(tf.keras.callbacks.ModelCheckpoint):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filepath,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=False,\n",
    "        mode=\"auto\",\n",
    "        verbose=0,\n",
    "        save_freq=\"epoch\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            filepath=filepath,\n",
    "            monitor=monitor,\n",
    "            save_best_only=save_best_only,\n",
    "            save_weights_only=save_weights_only,\n",
    "            mode=mode,\n",
    "            verbose=verbose,\n",
    "            save_freq=save_freq,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.step_count = 0\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.save_freq == 0:\n",
    "            self.save_weights(self.filepath, overwrite=True)\n",
    "\n",
    "\n",
    "class HistoryLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_interval):\n",
    "        super().__init__()\n",
    "        self.log_interval = log_interval\n",
    "        self.step_count = 0\n",
    "        self.history = []\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.log_interval == 0:\n",
    "            self.history.append(logs)\n",
    "\n",
    "    def get_history(self):\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateauSteps(\n",
    "    monitor=\"loss\", factor=0.5, mode=\"min\", patience=5000, verbose=1, min_lr=1e-8\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingSteps(\n",
    "    monitor=\"loss\",\n",
    "    min_delta=0,\n",
    "    patience=10000,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "checkpoint_dir = os.path.join(BASE_DIR, config[\"data\"][\"checkpoint_dir\"])\n",
    "if MODE == \"extrapol\":\n",
    "    checkpoint_path = os.path.join(\n",
    "        checkpoint_dir, f\"cvae_{DATA}_{MODE}_{PERCENTAGE}.h5\"\n",
    "    )\n",
    "elif MODE == \"interpol\":\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"cvae_{DATA}_{MODE}.h5\")\n",
    "\n",
    "# use ModelCheckpoint to save best model\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_best_only=True,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"auto\",\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "history_logger = HistoryLogger(log_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate=config[\"CVAE\"][\"learning_rate\"]\n",
    "    ),\n",
    "    loss=mse_kl_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "11265/11265 [==============================] - ETA: 0s - batch: 5632.0000 - size: 1.0000 - loss: 0.0952"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagociic/miniconda3/envs/cvae/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: loss improved from inf to 0.09516, saving model to /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/checkpoints/cvae_unet_extrapol_50.h5\n",
      "11265/11265 [==============================] - 1398s 124ms/step - batch: 5632.0000 - size: 1.0000 - loss: 0.0952 - val_loss: 0.5045 - lr: 3.0000e-04\n",
      "Epoch 2/2\n",
      "10631/11265 [===========================>..] - ETA: 1:19 - batch: 5315.0000 - size: 1.0000 - loss: 0.0561Reducing learning rate to 0.00015.\n",
      "11265/11265 [==============================] - ETA: 0s - batch: 5632.0000 - size: 1.0000 - loss: 0.0556\n",
      "Epoch 2: loss improved from 0.09516 to 0.05562, saving model to /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/checkpoints/cvae_unet_extrapol_50.h5\n",
      "11265/11265 [==============================] - 1408s 125ms/step - batch: 5632.0000 - size: 1.0000 - loss: 0.0556 - val_loss: 0.4769 - lr: 1.5000e-04\n"
     ]
    }
   ],
   "source": [
    "cvae.optimizer.lr = config[\"CVAE\"][\"learning_rate\"]\n",
    "\n",
    "# Fit the model\n",
    "history = cvae.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=len(train_data_gen),\n",
    "    epochs=epochs,\n",
    "    validation_data=test_data_gen,\n",
    "    validation_steps=len(test_data_gen),\n",
    "    callbacks=[reduce_lr, early_stopping, model_checkpoint, history_logger],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### VER HISTORY ####\n",
    "\n",
    "\n",
    "# plot and save learning curves\n",
    "# if MODE == \"extrapol\":\n",
    "#     save_history(\n",
    "#         history, os.path.join(checkpoint_dir, f\"history_{DATA}_{MODE}_{PERCENTAGE}.csv\")\n",
    "#     )\n",
    "    # plot_learning_curves(\n",
    "    #     history,\n",
    "    #     log_scale=True,\n",
    "    #     plt_title=f\"CVAE {DATA} {MODE} {PERCENTAGE}\",\n",
    "    #     save_fig=True,\n",
    "    # )\n",
    "\n",
    "# elif MODE == \"interpol\":\n",
    "#     save_history(history, os.path.join(checkpoint_dir, f\"history_{DATA}_{MODE}.csv\"))\n",
    "    # plot_learning_curves(\n",
    "    #     history, log_scale=True, plt_title=f\"CVAE_{DATA}_{MODE}\", save_fig=True\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "cvae.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frames(\n",
    "    decoder, output_dir: str, total_frames: int = 22500, resize_original: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates and saves the frames from a trained decoder.\n",
    "\n",
    "    Parameters:\n",
    "        decoder (keras.Model): The trained decoder.\n",
    "        output_dir (str): The path to the output directory.\n",
    "        total_frames (int): The total number of frames to generate.\n",
    "        resize_original (bool): Whether to resize the frames to the original dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    start_total_time = time.time()\n",
    "\n",
    "    frames_num = np.arange(1, total_frames + 1, 1)\n",
    "\n",
    "    for i in range(total_frames):\n",
    "        frame_num = frames_num[i]\n",
    "\n",
    "        # Sample from the latent space\n",
    "        z_sample = np.full((1, config[\"CVAE\"][\"latent_dim\"]), 0.5)\n",
    "\n",
    "        # Generate the frame\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            reconst = decoder.predict([z_sample, frame_to_label(frame_num)])\n",
    "            reconst_time = (time.time() - start_time) * 1000\n",
    "            reconst = np.squeeze(reconst, axis=0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating frame {frame_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if resize_original:\n",
    "            start_time = time.time()\n",
    "            reconst = tf.image.resize(\n",
    "                images=reconst, size=config[\"data\"][\"original_vid_dims\"]\n",
    "            )\n",
    "            resize_time = (time.time() - start_time) * 1000\n",
    "        else:\n",
    "            resize_time = 0.0  # Not resizing\n",
    "\n",
    "        # Binarize the reconstructed image with OpenCV\n",
    "        start_time = time.time()\n",
    "        _, thresh_img = cv2.threshold(\n",
    "            reconst, config[\"CVAE\"][\"threshold\"], 255, cv2.THRESH_BINARY\n",
    "        )\n",
    "        threshold_time = (time.time() - start_time) * 1000\n",
    "\n",
    "        # Save the thresholded image as png in grayscale\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            cv2.imwrite(\n",
    "                os.path.join(output_dir, f\"frame_{frame_num:06d}.png\"), thresh_img\n",
    "            )\n",
    "            save_time = (time.time() - start_time) * 1000\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving frame {frame_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Print progress with time information\n",
    "        print(\n",
    "            f\"Generated frame {i+1} of {total_frames} | \"\n",
    "            f\"Reconst: {reconst_time:.2f}ms | \"\n",
    "            f\"Resize: {resize_time:.2f}ms | \"\n",
    "            f\"Threshold: {threshold_time:.2f}ms | \"\n",
    "            f\"Save: {save_time:.2f}ms | \"\n",
    "            f\"Elapsed Time: {time.time() - start_total_time:.2f}s  \",\n",
    "            end=\"\\r\",\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagociic/miniconda3/envs/cvae/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated frame 22500 of 22500 | Reconst: 22.33ms | Resize: 0.00ms | Threshold: 0.09ms | Save: 0.91ms | Elapsed Time: 543.58s  \n"
     ]
    }
   ],
   "source": [
    "output_png_dir = os.path.join(output_dir, \"PNG\")\n",
    "generate_frames(decoder, output_png_dir, total_frames=LAST_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Creating image list...                          \n",
      "INFO - Writing frames to file 1/22500\n",
      "INFO - Writing frames to file 1001/22500\n",
      "INFO - Writing frames to file 2001/22500\n",
      "INFO - Writing frames to file 3001/22500\n",
      "INFO - Writing frames to file 4001/22500\n",
      "INFO - Writing frames to file 5001/22500\n",
      "INFO - Writing frames to file 6001/22500\n",
      "INFO - Writing frames to file 7001/22500\n",
      "INFO - Writing frames to file 8001/22500\n",
      "INFO - Writing frames to file 9001/22500\n",
      "INFO - Writing frames to file 10001/22500\n",
      "INFO - Writing frames to file 11001/22500\n",
      "INFO - Writing frames to file 12001/22500\n",
      "INFO - Writing frames to file 13001/22500\n",
      "INFO - Writing frames to file 14001/22500\n",
      "INFO - Writing frames to file 15001/22500\n",
      "INFO - Writing frames to file 16001/22500\n",
      "INFO - Writing frames to file 17001/22500\n",
      "INFO - Writing frames to file 18001/22500\n",
      "INFO - Writing frames to file 19001/22500\n",
      "INFO - Writing frames to file 20001/22500\n",
      "INFO - Writing frames to file 21001/22500\n",
      "INFO - Writing frames to file 22001/22500\n",
      "INFO - Saved video to /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/outputs/CVAE/extrapol/50/unet/video_unet_extrapol_50.mp4\n"
     ]
    }
   ],
   "source": [
    "# generate video from the generated frames\n",
    "if MODE == \"extrapol\":\n",
    "    file_name = f\"video_{DATA}_{MODE}_{PERCENTAGE}\"\n",
    "    title = f\"CVAE: {MODE}ation - {DATA}, {PERCENTAGE}, {config['CVAE']['epochs']} epochs, 10x speed\"\n",
    "elif MODE == \"interpol\":\n",
    "    file_name = f\"video_{DATA}_{MODE}\"\n",
    "    title = f\"CVAE: {MODE}ation - {DATA}, {config['CVAE']['epochs']} epochs, 10x speed\"\n",
    "\n",
    "frames_to_video(\n",
    "    img_list_dir=os.path.join(output_dir, \"PNG\"),\n",
    "    output_dir=output_dir,\n",
    "    output_resolution=config[\"data\"][\"original_vid_dims\"],\n",
    "    title=title,\n",
    "    f_ps=250,  # 10x speed\n",
    "    file_name=file_name,\n",
    "    frame_num_text=True,\n",
    "    font_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Converting masks to polygons...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22499 masks out of 22500 | Time elapsed: 7285.77s  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Saved polygons to /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/outputs/CVAE/extrapol/50/unet/WKT/extrapol_unet.wkt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22500 masks out of 22500 | Time elapsed: 7286.15s  \r"
     ]
    }
   ],
   "source": [
    "# List of generated frames paths\n",
    "msks_paths = sorted(glob(os.path.join(output_png_dir, \"*.png\")))\n",
    "\n",
    "# Convert the masks to polygons and save them as a WKT file\n",
    "masks_to_polygons(\n",
    "    msks_paths,\n",
    "    out_dim=tuple(config[\"data\"][\"original_vid_dims\"]),\n",
    "    save_path=os.path.join(BASE_DIR,\"outputs\", MODEL, MODE, str(PERCENTAGE), DATA, \"WKT\", f\"{MODE}_{DATA}.wkt\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
