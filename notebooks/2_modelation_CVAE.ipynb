{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Tensorflow Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Lambda, Input, Dense\n",
    "from tensorflow.keras.layers import Flatten, Reshape, Concatenate\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "\n",
    "# Local Module Import\n",
    "sys.path.append(\"../src\")  # adds source code directory\n",
    "from utils import frame_to_label, frames_to_video\n",
    "from polygon_handle import masks_to_polygons\n",
    "from log_setup import logger\n",
    "from cvae_model import CVAEDataGenerator, CVAEComponents\n",
    "from cvae_model import ReduceLROnPlateauSteps, EarlyStoppingSteps, HistoryLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "DATA: \"full\" (full dataset), \"sampled\" (distance sampled dataset) \n",
    "        or \"unet\" (unet generated dataset)\n",
    "MODE: \"interpol\" (interpolation) or \"extrapol\" (extrapolation)\n",
    "MODEL: \"CVAE\"\n",
    "PERCENTAGE: percentage of training data to be used for training\n",
    "LAST_FRAME: last frame number of the video\n",
    "\"\"\"\n",
    "\n",
    "DATA = \"unet\"\n",
    "MODE = \"extrapol\"\n",
    "MODEL = \"CVAE\"\n",
    "PERCENTAGE = 30\n",
    "LAST_FRAME = 22500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Data: unet, Mode: extrapol, Model: CVAE Percentage: 30%,\n",
      "Output directory: /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/outputs/CVAE/extrapol/30/unet\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "dataset_dir = os.path.join(BASE_DIR, \"dataset\")\n",
    "data_dir = os.path.join(BASE_DIR, \"data\")\n",
    "config_file = os.path.join(BASE_DIR, \"config.yml\")\n",
    "\n",
    "# Output PNG directory\n",
    "if MODE == \"extrapol\":\n",
    "    output_dir = os.path.join(BASE_DIR, \"outputs\", \"CVAE\", MODE, str(PERCENTAGE), DATA)\n",
    "    logger.info(\n",
    "        f\"Data: {DATA}, Mode: {MODE}, Model: {MODEL} Percentage: {PERCENTAGE}%,\\nOutput directory: {output_dir}\"\n",
    "    )\n",
    "elif MODE == \"interpol\":\n",
    "    output_dir = os.path.join(BASE_DIR, \"outputs\", \"CVAE\", MODE, DATA)\n",
    "    logger.info(\n",
    "        f\"\\nData: {DATA}, Mode: {MODE}, Model: {MODEL}\\nOutput directory: {output_dir}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - No. train. samples: 6759 out of 22500 (30%) | No. test samples: 23\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "if DATA == \"full\":\n",
    "    train_dir = os.path.join(BASE_DIR, config[\"data\"][\"full\"][\"train_dir\"], \"masks\")\n",
    "    # sort the paths\n",
    "    train_paths = sorted(glob(os.path.join(train_dir, \"*.png\")))\n",
    "    # extract labels from the paths\n",
    "    train_labels = [\n",
    "        int(os.path.basename(m).split(\"_\")[1].split(\".\")[0]) * 100 for m in train_paths\n",
    "    ]\n",
    "    epochs = config[\"CVAE\"][\"epochs\"]\n",
    "\n",
    "elif DATA == \"sampled\":\n",
    "    sampled_masks_txt_path = os.path.join(\n",
    "        BASE_DIR, config[\"data\"][\"wkt\"][\"sampled_masks_txt\"]\n",
    "    )\n",
    "    with open(sampled_masks_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        polygons = f.readlines()\n",
    "        # extract indexes\n",
    "    indexes = [int(polygon.split(\",\")[0]) for polygon in polygons]\n",
    "    train_dir = os.path.join(BASE_DIR, config[\"data\"][\"sampled\"][\"train_dir\"], \"masks\")\n",
    "    train_paths = sorted(glob(os.path.join(train_dir, \"*.png\")))\n",
    "    train_labels = [100 * i for i in indexes]\n",
    "    epochs = config[\"CVAE\"][\"epochs\"]\n",
    "\n",
    "elif DATA == \"unet\":\n",
    "    train_dir = os.path.join(BASE_DIR, config[\"data\"][\"unet\"][\"train_dir\"], \"masks\")\n",
    "    train_paths = sorted(glob(os.path.join(train_dir, \"*.png\")))\n",
    "    train_labels = [\n",
    "        int(os.path.basename(m).split(\"_\")[1].split(\".\")[0]) for m in train_paths\n",
    "    ]\n",
    "    epochs = 2\n",
    "\n",
    "\n",
    "# Test data\n",
    "test_dir = os.path.join(BASE_DIR, config[\"data\"][\"test\"][\"test_dir\"], \"masks\")\n",
    "test_paths = sorted(glob(os.path.join(test_dir, \"*.png\")))\n",
    "test_labels = [\n",
    "    int(os.path.basename(m).split(\"_\")[1].split(\".\")[0]) * 100 + 20250\n",
    "    for m in test_paths\n",
    "]\n",
    "\n",
    "if MODE == \"extrapol\":\n",
    "    # Truncate the training data\n",
    "    train_paths = train_paths[: int(len(train_paths) * PERCENTAGE / 100)]\n",
    "    train_labels = train_labels[: int(len(train_labels) * PERCENTAGE / 100)]\n",
    "    logger.info(\n",
    "        f\"No. train. samples: {len(train_paths)} out of {LAST_FRAME} ({PERCENTAGE}%) | No. test samples: {len(test_paths)}\"\n",
    "    )\n",
    "elif MODE == \"interpol\":\n",
    "    logger.info(\n",
    "        f\"No. train. samples: {len(train_paths)} out of {LAST_FRAME} | No. test samples: {len(test_paths)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sampled_masks_txt_path = os.path.join(BASE_DIR, config[\"data\"][\"wkt\"][\"sampled_masks_txt\"])\n",
    "except KeyError:\n",
    "    print(\"Key 'sampled_masks_txt' not found in the config data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = config[\"CVAE\"][\"input_shape\"]\n",
    "\n",
    "# Create training data generator\n",
    "train_data_gen = CVAEDataGenerator(\n",
    "    data_paths=train_paths,\n",
    "    labels=train_labels,\n",
    "    batch_size=1,\n",
    "    input_shape=input_shape[:2],\n",
    "    last_frame=LAST_FRAME,\n",
    ")\n",
    "\n",
    "# Create testing data generator\n",
    "test_data_gen = CVAEDataGenerator(\n",
    "    data_paths=test_paths,\n",
    "    labels=test_labels,\n",
    "    batch_size=1,\n",
    "    input_shape=input_shape[:2],\n",
    "    last_frame=LAST_FRAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-VAE definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cvae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 512, 512, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        [(None, 64),                 3357281   ['input_2[0][0]']             \n",
      "                              (None, 64),                 6                                       \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " label (InputLayer)          [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, 512, 512, 1)          1749779   ['encoder[0][2]',             \n",
      "                                                          3          'label[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 51070609 (194.82 MB)\n",
      "Trainable params: 51070609 (194.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "H, W, C = config[\"CVAE\"][\"input_shape\"]\n",
    "filters = int(config[\"CVAE\"][\"ref_filters\"])\n",
    "cvae_comp = CVAEComponents()\n",
    "\n",
    "\n",
    "# --------\n",
    "# Encoder\n",
    "# --------\n",
    "\n",
    "encoder_inputs = Input(shape=(H, W, C))\n",
    "# Reshape input to 2D image\n",
    "\n",
    "x = cvae_comp.conv_block(\n",
    "    input=encoder_inputs, filters=filters * 2, f_init=config[\"CVAE\"][\"w_init\"]\n",
    ")\n",
    "x = cvae_comp.conv_block(input=x, filters=filters, f_init=config[\"CVAE\"][\"w_init\"])\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation=\"leaky_relu\")(x)\n",
    "\n",
    "# VAE specific layers for mean and log variance\n",
    "z_mean = Dense(config[\"CVAE\"][\"latent_dim\"], activation=\"leaky_relu\", name=\"z_mean\")(x)\n",
    "z_log_var = Dense(\n",
    "    config[\"CVAE\"][\"latent_dim\"], activation=\"leaky_relu\", name=\"z_log_var\"\n",
    ")(x)\n",
    "\n",
    "# Sampling layer to sample z from the latent space\n",
    "z = Lambda(cvae_comp.sampler, output_shape=(config[\"CVAE\"][\"latent_dim\"],), name=\"z\")(\n",
    "    [z_mean, z_log_var]\n",
    ")\n",
    "\n",
    "# Instantiate encoder model\n",
    "encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "# --------\n",
    "# Decoder\n",
    "# --------\n",
    "\n",
    "latent_inputs = Input(shape=(config[\"CVAE\"][\"latent_dim\"],), name=\"z_sampling\")\n",
    "label_size = 1  # one tf.float32 label\n",
    "label_inputs = Input(shape=(label_size,), name=\"label\")\n",
    "decoder_inputs = Concatenate()([latent_inputs, label_inputs])\n",
    "x = Dense(64 * 64 * 64, activation=\"leaky_relu\")(decoder_inputs)\n",
    "x = Reshape((128, 128, 16))(x)\n",
    "x = cvae_comp.deconv_block(input= x, filters= filters * 2, f_init = config[\"CVAE\"][\"w_init\"])\n",
    "x = cvae_comp.deconv_block(input= x, filters=filters * 4, f_init = config[\"CVAE\"][\"w_init\"])\n",
    "decoder_output = Conv2DTranspose(1, 3, activation=\"tanh\", padding=\"same\")(x)\n",
    "\n",
    "decoder = Model([latent_inputs, label_inputs], decoder_output, name=\"decoder\")\n",
    "\n",
    "# -----------------\n",
    "# Conditional VAE\n",
    "# -----------------\n",
    "\n",
    "outputs = decoder([encoder(encoder_inputs)[2], label_inputs])\n",
    "cvae = Model([encoder_inputs, label_inputs], outputs, name=\"cvae\")\n",
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateauSteps(\n",
    "    monitor=\"loss\", factor=0.5, mode=\"min\", patience=5000, verbose=1, min_lr=1e-8\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingSteps(\n",
    "    monitor=\"loss\",\n",
    "    min_delta=0,\n",
    "    patience=10000,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "checkpoint_dir = os.path.join(BASE_DIR, config[\"data\"][\"checkpoint_dir\"])\n",
    "if MODE == \"extrapol\":\n",
    "    checkpoint_path = os.path.join(\n",
    "        checkpoint_dir, f\"cvae_{DATA}_{MODE}_{PERCENTAGE}.h5\"\n",
    "    )\n",
    "elif MODE == \"interpol\":\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"cvae_{DATA}_{MODE}.h5\")\n",
    "\n",
    "# use ModelCheckpoint to save best model\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_best_only=True,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"auto\",\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "history_logger = HistoryLogger(log_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate=config[\"CVAE\"][\"learning_rate\"]\n",
    "    ),\n",
    "    loss= cvae_comp.mse_kl_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1848/6759 [=======>......................] - ETA: 8:25 - batch: 923.5000 - size: 1.0000 - loss: 0.1135"
     ]
    }
   ],
   "source": [
    "cvae.optimizer.lr = config[\"CVAE\"][\"learning_rate\"]\n",
    "\n",
    "# Fit the model\n",
    "history = cvae.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=len(train_data_gen),\n",
    "    epochs=2,\n",
    "    validation_data=test_data_gen,\n",
    "    validation_steps=len(test_data_gen),\n",
    "    callbacks=[reduce_lr, early_stopping,history_logger],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "cvae.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frames(\n",
    "    decoder, output_dir: str, total_frames: int = 22500, resize_original: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates and saves the frames from a trained decoder.\n",
    "\n",
    "    Parameters:\n",
    "        decoder (keras.Model): The trained decoder.\n",
    "        output_dir (str): The path to the output directory.\n",
    "        total_frames (int): The total number of frames to generate.\n",
    "        resize_original (bool): Whether to resize the frames to the original dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    start_total_time = time.time()\n",
    "\n",
    "    frames_num = np.arange(1, total_frames + 1, 1)\n",
    "\n",
    "    for i in range(total_frames):\n",
    "        frame_num = frames_num[i]\n",
    "\n",
    "        # Sample from the latent space\n",
    "        z_sample = np.full((1, config[\"CVAE\"][\"latent_dim\"]), 0.5)\n",
    "\n",
    "        # Generate the frame\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            reconst = decoder.predict([z_sample, frame_to_label(frame_num)])\n",
    "            reconst_time = (time.time() - start_time) * 1000\n",
    "            reconst = np.squeeze(reconst, axis=0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating frame {frame_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if resize_original:\n",
    "            start_time = time.time()\n",
    "            reconst = tf.image.resize(\n",
    "                images=reconst, size=config[\"data\"][\"original_vid_dims\"]\n",
    "            )\n",
    "            resize_time = (time.time() - start_time) * 1000\n",
    "        else:\n",
    "            resize_time = 0.0  # Not resizing\n",
    "\n",
    "        # Binarize the reconstructed image with OpenCV\n",
    "        start_time = time.time()\n",
    "        _, thresh_img = cv2.threshold(\n",
    "            reconst, config[\"CVAE\"][\"threshold\"], 255, cv2.THRESH_BINARY\n",
    "        )\n",
    "        threshold_time = (time.time() - start_time) * 1000\n",
    "\n",
    "        # Save the thresholded image as png in grayscale\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            cv2.imwrite(\n",
    "                os.path.join(output_dir, f\"frame_{frame_num:06d}.png\"), thresh_img\n",
    "            )\n",
    "            save_time = (time.time() - start_time) * 1000\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving frame {frame_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Print progress with time information\n",
    "        print(\n",
    "            f\"Generated frame {i+1} of {total_frames} | \"\n",
    "            f\"Reconst: {reconst_time:.2f}ms | \"\n",
    "            f\"Resize: {resize_time:.2f}ms | \"\n",
    "            f\"Threshold: {threshold_time:.2f}ms | \"\n",
    "            f\"Save: {save_time:.2f}ms | \"\n",
    "            f\"Elapsed Time: {time.time() - start_total_time:.2f}s  \",\n",
    "            end=\"\\r\",\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagociic/miniconda3/envs/cvae/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated frame 22500 of 22500 | Reconst: 23.82ms | Resize: 0.00ms | Threshold: 0.17ms | Save: 0.87ms | Elapsed Time: 540.08s  \n"
     ]
    }
   ],
   "source": [
    "output_png_dir = os.path.join(output_dir, \"PNG\")\n",
    "generate_frames(decoder, output_png_dir, total_frames=LAST_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Creating image list...                          \n",
      "INFO - Writing frames to file 1/22500\n",
      "INFO - Writing frames to file 1001/22500\n",
      "INFO - Writing frames to file 2001/22500\n",
      "INFO - Writing frames to file 3001/22500\n",
      "INFO - Writing frames to file 4001/22500\n",
      "INFO - Writing frames to file 5001/22500\n",
      "INFO - Writing frames to file 6001/22500\n",
      "INFO - Writing frames to file 7001/22500\n",
      "INFO - Writing frames to file 8001/22500\n",
      "INFO - Writing frames to file 9001/22500\n",
      "INFO - Writing frames to file 10001/22500\n",
      "INFO - Writing frames to file 11001/22500\n",
      "INFO - Writing frames to file 12001/22500\n",
      "INFO - Writing frames to file 13001/22500\n",
      "INFO - Writing frames to file 14001/22500\n",
      "INFO - Writing frames to file 15001/22500\n",
      "INFO - Writing frames to file 16001/22500\n",
      "INFO - Writing frames to file 17001/22500\n",
      "INFO - Writing frames to file 18001/22500\n",
      "INFO - Writing frames to file 19001/22500\n",
      "INFO - Writing frames to file 20001/22500\n",
      "INFO - Writing frames to file 21001/22500\n",
      "INFO - Writing frames to file 22001/22500\n",
      "INFO - Saved video to /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/outputs/CVAE/extrapol/30/unet/video_unet_extrapol_30.mp4\n"
     ]
    }
   ],
   "source": [
    "# generate video from the generated frames\n",
    "if MODE == \"extrapol\":\n",
    "    file_name = f\"video_{DATA}_{MODE}_{PERCENTAGE}\"\n",
    "    title = f\"CVAE: {MODE}ation - {DATA}, {PERCENTAGE}, {config['CVAE']['epochs']} epochs, 10x speed\"\n",
    "elif MODE == \"interpol\":\n",
    "    file_name = f\"video_{DATA}_{MODE}\"\n",
    "    title = f\"CVAE: {MODE}ation - {DATA}, {config['CVAE']['epochs']} epochs, 10x speed\"\n",
    "\n",
    "frames_to_video(\n",
    "    img_list_dir=os.path.join(output_dir, \"PNG\"),\n",
    "    output_dir=output_dir,\n",
    "    output_resolution=config[\"data\"][\"original_vid_dims\"],\n",
    "    title=title,\n",
    "    f_ps=250,  # 10x speed\n",
    "    file_name=file_name,\n",
    "    frame_num_text=True,\n",
    "    font_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Converting masks to polygons...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22499 masks out of 22500 | Time elapsed: 4408.34s  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Saved polygons to /home/tiagociic/Projectos/spatiotemporal-vae-reconstruction/outputs/CVAE/extrapol/30/unet/WKT/extrapol_unet.wkt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22500 masks out of 22500 | Time elapsed: 4408.57s  \r"
     ]
    }
   ],
   "source": [
    "# List of generated frames paths\n",
    "msks_paths = sorted(glob(os.path.join(output_png_dir, \"*.png\")))\n",
    "\n",
    "# Convert the masks to polygons and save them as a WKT file\n",
    "masks_to_polygons(\n",
    "    msks_paths,\n",
    "    out_dim=tuple(config[\"data\"][\"original_vid_dims\"]),\n",
    "    save_path=os.path.join(BASE_DIR,\"outputs\", MODEL, MODE, str(PERCENTAGE), DATA, \"WKT\", f\"{MODE}_{DATA}.wkt\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
